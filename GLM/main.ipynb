{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ed7e99",
   "metadata": {},
   "source": [
    "# Generalized Linear Model (GLM)\n",
    "\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b9cc2",
   "metadata": {},
   "source": [
    "Links for understand topic:\n",
    "* Stanford CS299: https://www.youtube.com/watch?v=iZTeva0WSTQ&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e57cf",
   "metadata": {},
   "source": [
    "The main idea of knowing and using **Generalized Linear Models (GLMs)** is that we are able to solve many classic machine learning problems using basic linear regression principles.\n",
    "\n",
    "As I understand it, we train a linear regression model and then use a **link function** to transform its output so that it matches the **distribution of the target variable**.\n",
    "\n",
    "This topic shows us how functions we already know, such as sigmoid, softmax, and others, were created and why they are used.\n",
    "\n",
    "That is why this topic should be one of **the core theorems** (or core topics) in classical machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc5b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a7d133",
   "metadata": {},
   "source": [
    "## Optimize parameters by Gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b1bbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_update(X, y, mu):\n",
    "    return X.T @ (y - mu) # That's all!\n",
    "\n",
    "# X.T @ theta => eta => mu\n",
    "\n",
    "# 1) Linear Regression | mu = eta = X @ theta\n",
    "def linear_mu(X, theta):\n",
    "    mu = X @ theta\n",
    "    return mu\n",
    "\n",
    "# 2) Logistic regression | mu = 1/(1+exp(-eta)) = 1/(1+exp(-X @ theta))\n",
    "def logistic_mu(X, theta):\n",
    "    eta = X @ theta\n",
    "    eta = np.clip(eta, -500, 500)\n",
    "    mu = 1/(1+np.exp(-(eta)))\n",
    "    return mu\n",
    "\n",
    "# 3) Softmax regression | mu_i = (exp(X @ theta_i)) / sum(exp(X @ theta_j))\n",
    "# y.shape == mu.shape == (n_samples, n_classes) - Main condition\n",
    "def softmax_mu(X, theta):\n",
    "    logits = X @ theta\n",
    "    logits -= np.max(logits, axis=1, keepdims=True)  # стабилизация\n",
    "    # logits = np.hstack([logits, np.zeros((X.shape[0], 1))])\n",
    "    exp_logits = np.exp(logits)\n",
    "    mu = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ee2fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common stuctured def for optimize parameters\n",
    "def fit(X, theta_init, y, task, lr=1e-3, n_iters=1000):\n",
    "    theta = theta_init.copy()\n",
    "    \n",
    "    for _ in range(n_iters):\n",
    "        if task == 'linear':\n",
    "            mu = linear_mu(X, theta)\n",
    "        elif task == 'logistic':\n",
    "            mu = logistic_mu(X, theta)\n",
    "        elif task == 'softmax':\n",
    "            mu = softmax_mu(X, theta)\n",
    "            # y must be one-hot here!\n",
    "        else:\n",
    "            raise ValueError(\"Unknown task\")\n",
    "        \n",
    "        theta += lr * grad_update(X, y, mu)\n",
    "        \n",
    "        # if task == 'softmax':\n",
    "        #     theta[:, 0] = 0\n",
    "    \n",
    "    return theta     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2977e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate X (data)\n",
    "# y = X @ Theta + epsilone\n",
    "\n",
    "def make_X(n_samples=1000, n_features=3, bias=True, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(size=(n_samples, n_features))\n",
    "    if bias:\n",
    "        X = np.c_[np.ones(n_samples), X]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a4db4",
   "metadata": {},
   "source": [
    "### Linear Regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e309a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta (TRUE) = [3.2678, 1.365, 0.653, -0.0698]\n",
      "Theta (AFTER TRAIN) = [ 3.21293235  1.30052909  0.68080355 -0.05252388]\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression by GLM:\n",
    "def generate_linear_data(X, theta, sigma=1.0, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    mu = X @ theta\n",
    "    y = mu + rng.normal(scale=sigma, size=mu.shape)\n",
    "    return y\n",
    "\n",
    "# Generate data for Linear Regression problem:\n",
    "X = make_X()\n",
    "theta_true = [3.2678, 1.365, 0.653, -0.0698]\n",
    "y_true = generate_linear_data(X, theta_true)\n",
    "\n",
    "# Let's go use GLM!!!\n",
    "theta_0 = [1, 1, 1, 1]\n",
    "theta_train = fit(X, theta_0, y_true, 'linear')\n",
    "print(f\"Theta (TRUE) = {theta_true}\")\n",
    "print(f\"Theta (AFTER TRAIN) = {theta_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df809b45",
   "metadata": {},
   "source": [
    "### Logistic Regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8baeec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta (TRUE) = [3.2678, 1.365, 0.653, -0.0698]\n",
      "Theta (AFTER TRAIN) = [3.1325853  1.38198022 0.52704664 0.11608256]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression by GLM:\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def generate_logistic_data(X, theta, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    eta = X @ theta\n",
    "    mu = sigmoid(eta)\n",
    "    y = rng.binomial(1, mu)\n",
    "    return y\n",
    "\n",
    "# Generate data for Logistic Regression problem:\n",
    "X = make_X()\n",
    "theta_true = [3.2678, 1.365, 0.653, -0.0698]\n",
    "y_true = generate_logistic_data(X, theta_true)\n",
    "\n",
    "# Let's go use GLM!!!\n",
    "theta_0 = [1, 1, 1, 1]\n",
    "theta_train = fit(X, theta_0, y_true, 'logistic', n_iters=2000)\n",
    "print(f\"Theta (TRUE) = {theta_true}\")\n",
    "print(f\"Theta (AFTER TRAIN) = {theta_train}\")\n",
    "\n",
    "# I understand that theta_true != theta_train is normal thing. For solve this I need to generate more data and use regulirization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275f27c",
   "metadata": {},
   "source": [
    "### Softmax Regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ca898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta (TRUE) = \n",
      "[[ 3.  -1.   0.5]\n",
      " [ 1.2  0.3 -0.8]\n",
      " [ 0.7 -0.4  0.2]\n",
      " [-0.1  0.6 -0.3]]\n",
      "\n",
      "Theta (AFTER TRAIN) = \n",
      "[[ 3.04584903e+00 -7.28946582e-01  6.83097551e-01]\n",
      " [ 1.99526300e+00  1.00735078e+00 -2.61377540e-03]\n",
      " [ 1.44649430e+00  4.79942781e-01  1.07356292e+00]\n",
      " [ 9.05299805e-01  1.56767055e+00  5.27029648e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Softmax Regression by GLM:\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def generate_softmax_data(X, theta, seed=0):\n",
    "    \"\"\"\n",
    "    theta: (n_features, n_classes)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    logits = X @ theta\n",
    "    mu = softmax(logits)\n",
    "\n",
    "    y = np.array([\n",
    "        rng.choice(mu.shape[1], p=mu[i])\n",
    "        for i in range(mu.shape[0])\n",
    "    ])\n",
    "\n",
    "    # one-hot\n",
    "    y_onehot = np.eye(mu.shape[1])[y]\n",
    "    return y, y_onehot\n",
    "\n",
    "# Generate data for Softmax Regression problem:\n",
    "X = make_X()\n",
    "theta_true = np.array([\n",
    "    [ 3.0, -1.0,  0.5],\n",
    "    [ 1.2,  0.3, -0.8],\n",
    "    [ 0.7, -0.4,  0.2],\n",
    "    [-0.1,  0.6, -0.3],\n",
    "]) \n",
    "y_true, y_true_onehot = generate_softmax_data(X, theta_true)\n",
    "\n",
    "\n",
    "# Let's go use GLM!!!\n",
    "theta_0 = np.array([\n",
    "    [ 1.0, 1.0, 1.0 ],\n",
    "    [ 1.0, 1.0, 1.0 ],\n",
    "    [ 1.0, 1.0, 1.0 ],\n",
    "    [ 1.0, 1.0, 1.0 ],\n",
    "]) \n",
    "theta_train = fit(X, theta_0, y_true_onehot, 'softmax', n_iters=1000)\n",
    "print(f\"Theta (TRUE) = \\n{theta_true}\\n\")\n",
    "print(f\"Theta (AFTER TRAIN) = \\n{theta_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6ee9b",
   "metadata": {},
   "source": [
    "Today we talked about **how use GLM**, and if you study basic theory about it you understand that it is **cool thing** and **where we get all this formulations**! **In conclusion**, maybe someone have questions *\"HOW?\"* or *\"WHY?\"*, and **that's OK!** Because We did something at first time and did't know nothing.**However in one day** you will be **master** in it! **Keep moving!** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
